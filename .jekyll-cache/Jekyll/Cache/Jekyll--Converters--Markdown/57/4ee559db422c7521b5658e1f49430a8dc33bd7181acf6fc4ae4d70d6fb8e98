I"¡U<h2 id="data-cleaning-and-aggregation-in-the-difm-project">Data cleaning and aggregation in the DIFM project</h2>

<p>After harvesting, we collect all the data needed for analysis, and in advance of
running analysis, we clean and organize the data in order to remove machinary
error and such. The common data that we collect for analysis includes yield
(dry), seeding rate as-planted, nitrogen rate as-applied, electronic
conductivity (EC), SSURGO, soil test, weather, etc. In particular, we need to
clean yield data, as-planted data, as-applied data, and sometimes EC data. For
public data, we simply import them into our aggregated data set
without cleaning, since they have already been cleaned before being released to
the public.</p>

<h2 id="introduction-to-data-cleaning">Introduction to data cleaning</h2>

<p>Data cleaning is the process of removing or correcting errors in a dataset, and
is very important to do before any sort of analysis.  For example, say you were
manually entering yield values into a spreadsheet, and then wanted to take the
average of all values entered.  If you accidentally typed an extra zero into
some of the cells, the average that you calculate is going to be much higher
than the true average.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">real_data</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">900</span><span class="p">,</span><span class="w"> </span><span class="m">450</span><span class="p">,</span><span class="w"> </span><span class="m">200</span><span class="p">,</span><span class="w"> </span><span class="m">320</span><span class="p">)</span><span class="w">
</span><span class="n">error_data</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">900</span><span class="p">,</span><span class="w"> </span><span class="m">4500</span><span class="p">,</span><span class="w"> </span><span class="m">200</span><span class="p">,</span><span class="w"> </span><span class="m">320</span><span class="p">)</span><span class="w">
</span><span class="n">mean</span><span class="p">(</span><span class="n">real_data</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 467.5
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mean</span><span class="p">(</span><span class="n">error_data</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 1480
</code></pre></div></div>

<p>Therefore, we want to check for values like this before we do anything else.  If
the values were manually entered and the intended value is obvious, they can be
manually corrected.  For larger scale datasets, however, it is often most
practical to discard problematic data.</p>

<p>For example, we can plot our <code class="highlighter-rouge">error_data</code> and look for values that may look off:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot</span><span class="p">(</span><span class="n">error_data</span><span class="p">)</span><span class="w"> </span><span class="c1"># use plot function on error rate</span><span class="w">
</span></code></pre></div></div>

<p><img src="../fig/rmd-unnamed-chunk-2-1.png" title="plot of chunk unnamed-chunk-2" alt="plot of chunk unnamed-chunk-2" width="612" style="display: block; margin: auto;" />
By eye we can see the 2nd measurement (at <code class="highlighter-rouge">index = 2</code>) looks a little fishy.  In this case
we might want to apply a cut-off in our data so that we ignore all measurements above a
certain threshold when we do calculations like taking the mean of our data.</p>

<p>One way to do this is by setting any ‚Äúweird‚Äù values to <code class="highlighter-rouge">NA</code>:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">error_data</span><span class="p">[</span><span class="n">error_data</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="m">2000</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="kc">NA</span><span class="w"> </span><span class="c1"># set any values bigger than 2000 to the NA tag</span><span class="w">
</span><span class="n">error_data</span><span class="w">
</span></code></pre></div></div>

<div class="output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 900  NA 200 320
</code></pre></div></div>

<p>Now we can take a mean, with removing <code class="highlighter-rouge">NA</code>‚Äôs as we do it and recover a mean that is closer to the correct value:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mean</span><span class="p">(</span><span class="n">error_data</span><span class="p">,</span><span class="w"> </span><span class="n">na.rm</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 473.3333
</code></pre></div></div>

<p>Data cleaning is a major reason why there needs to be good communication between
data scientists and end users, in agriculture or any other discipline.  As the person
who generates the data, you know best where the likely sources of error might be.
Those sources of error might be something that someone who sits behind a computer
all day would never think of.  You also know best what values are reasonable,
and what values are suspiciously high or low.</p>

<p>For different types of data, we have different ways to clean them. Here are the
main concerns of the original data for the major variables:</p>

<p>Yield, as-planted, and as-applied data:</p>

<ul>
  <li>We remove observations where the harvester/planter/applicator is moving too slow or too fast.</li>
  <li>We remove observations on the edges of the plot.</li>
  <li>We remove observations that are below or above three standard deviations from the mean.</li>
  <li>We then aggregate them onto our units of observation. <font color="magenta">Do they know what these units are? I don't have a frame of refrence for this terminology</font></li>
</ul>

<font color="magenta">This comes up in the 03 or 04 but we need to define what a standard devation is</font>

<p><em>For aggregation, we need to generate subplots (units of observation) of the
original trial design, and then aggregate the cleaned datasets for different
variables onto the subplots.  Once we have one value per variable per subplot,
we can begin examining the relationships between the variables.</em></p>

<font color="magenta">Have steps listed somewhere before we start</font>

<h2 id="step-1-importing-and-transforming-our-shapefile-datasets">Step 1: Importing and transforming our shapefile datasets</h2>

<!-- Let's apply this data-cleaning thinking to measurements of yields.  First, we will aggregate our yields into a grid overlayed on top of our boundary shapefile and look for measurements on this grid that seem too low or to high, and flag these as "outliers" of our dataset in our data-cleaning process. -->

<p>The first step is to read in our boundary and abline shape files and transform them to UTM for later use.  Let‚Äôs do this step-by-step, starting with reading in the boundary shapefile and projecting it:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">boundary</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read_sf</span><span class="p">(</span><span class="s2">"data/boundary.gpkg"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>
<p>What is the current coordinate reference system of this object?</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">st_crs</span><span class="p">(</span><span class="n">boundary</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Coordinate Reference System:
  EPSG: 4326 
  proj4string: "+proj=longlat +datum=WGS84 +no_defs"
</code></pre></div></div>
<p>Let‚Äôs transform it to the UTM projection &amp; check out its new coordinate reference system:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">boundary_utm</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">st_transform_utm</span><span class="p">(</span><span class="n">boundary</span><span class="p">)</span><span class="w">
</span><span class="n">st_crs</span><span class="p">(</span><span class="n">boundary_utm</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Coordinate Reference System:
  EPSG: 32617 
  proj4string: "+proj=utm +zone=17 +datum=WGS84 +units=m +no_defs"
</code></pre></div></div>
<p>Now we can see that the <code class="highlighter-rouge">+proj=longlat</code> has changed to <code class="highlighter-rouge">+proj=utm</code> and gives us that we are in UTM zone #17.</p>

<p>In the last episode, we also imported our trial design, which we will do again here:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">trial</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read_sf</span><span class="p">(</span><span class="s2">"data/trial.gpkg"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>Let‚Äôs look at the coordinate reference system here as well:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">st_crs</span><span class="p">(</span><span class="n">trial</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Coordinate Reference System:
  EPSG: 32617 
  proj4string: "+proj=utm +zone=17 +datum=WGS84 +units=m +no_defs"
</code></pre></div></div>
<font color="magenta">Do we have a figure showing lat/long to UTM coversion somewhere?  I can add this</font>

<p>Our file is already in the UTM projection, but if we have one that is not we can convert this as well with <code class="highlighter-rouge">trial_utm &lt;- st_transform_utm(trial)</code>.  For the sake of naming, we‚Äôll rename it as <code class="highlighter-rouge">trial_utm</code>:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">trial_utm</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">trial</span><span class="w">
</span></code></pre></div></div>

<blockquote class="challenge">
  <h2 id="exercise-transform-the-yield-data">Exercise: Transform the yield data</h2>
  <p>Read in the yield shape file, look at its current CRS and transform it into the UTM projection.  Call this new, transformed variable <code class="highlighter-rouge">yield_utm</code>.</p>

  <blockquote class="solution">
    <h2 id="solution">Solution</h2>
    <p>First, load the data:</p>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">yield</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read_sf</span><span class="p">(</span><span class="s2">"data/yield.gpkg"</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>
    <p>Then take a look at the coordinate system:</p>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">st_crs</span><span class="p">(</span><span class="n">yield</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Coordinate Reference System:
  EPSG: 4326 
  proj4string: "+proj=longlat +datum=WGS84 +no_defs"
</code></pre></div>    </div>
    <p>And finally transform into UTM:</p>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">yield_utm</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">st_transform_utm</span><span class="p">(</span><span class="n">yield</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

  </blockquote>
</blockquote>

<p>Finally, let‚Äôs transform our abline file.  We read in the file:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">abline</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">st_read</span><span class="p">(</span><span class="s2">"data/abline.gpkg"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Reading layer `abline' from data source `/Users/jillnaiman/trial-lesson_ag/_episodes_rmd/data/abline.gpkg' using driver `GPKG'
Simple feature collection with 1 feature and 1 field
geometry type:  LINESTRING
dimension:      XY
bbox:           xmin: -82.87334 ymin: 40.84301 xmax: -82.87322 ymax: 40.84611
epsg (SRID):    4326
proj4string:    +proj=longlat +datum=WGS84 +no_defs
</code></pre></div></div>
<p>Check out its current coordinate reference system:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">st_crs</span><span class="p">(</span><span class="n">abline</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Coordinate Reference System:
  EPSG: 4326 
  proj4string: "+proj=longlat +datum=WGS84 +no_defs"
</code></pre></div></div>
<p>And transform it to UTM:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">abline_utm</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">st_transform_utm</span><span class="p">(</span><span class="n">abline</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<h2 id="step-2-clean-the-yield-data">Step 2: Clean the yield data</h2>

<p>Now that we have our shapefiles in the same UTM coordinate system reference frame, we will apply some of our knowledget of data cleaning to take out weird observations. We know we have ‚Äúweird‚Äù measurements by looking at a histogram of our yield data:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">hist</span><span class="p">(</span><span class="n">yield_utm</span><span class="o">$</span><span class="n">Yld_Vol_Dr</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="../fig/rmd-unnamed-chunk-14-1.png" title="plot of chunk unnamed-chunk-14" alt="plot of chunk unnamed-chunk-14" width="612" style="display: block; margin: auto;" /></p>

<p>The fact that this histogram has a large tail where we see a few measurements far beyond the majority around 250 means we know we have some weird data points.</p>

<p>We will take out these weird observations in two steps:</p>
<ol>
  <li>First, we will take out observations we <em>know</em> will be weird because they are taken from the edges of our plot.</li>
  <li>Second, we will take out observations that are too far away from where the majority of the other yield measurements lie.</li>
</ol>

<p>Let‚Äôs go through these one by one.</p>

<h3 id="data-cleaning-1-taking-out-boarder-observations">Data cleaning #1: Taking out boarder observations</h3>

<p>We need to remove the yield observations that are on the border of the plots,
and also at the end of the plots.  The reason for this is that along the edge
of a plot, the harvester is likely to measure a mixture of two plots,
and therefore the data won‚Äôt be accurate for either plot.  Additionally,
plants growing at the edge of the field are likely to suffer from wind and other
effects, lowering their yields.</p>

<p>We add a 15 <font color="magenta">meter??</font> boarder.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">yield_clean_boarder</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">clean_buffer</span><span class="p">(</span><span class="n">trial_utm</span><span class="p">,</span><span class="w"> </span><span class="m">15</span><span class="p">,</span><span class="w"> </span><span class="n">yield_utm</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>Let‚Äôs use our side-by-side plotting we did in the previous episode to compare our original and boarder-yield cleaned yield maps:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">yield_plot_orig</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">map_points</span><span class="p">(</span><span class="n">yield_utm</span><span class="p">,</span><span class="w"> </span><span class="s2">"Yld_Vol_Dr"</span><span class="p">,</span><span class="w"> </span><span class="s2">"Yield, Orig"</span><span class="p">)</span><span class="w">
</span><span class="n">yield_plot_boarder_cleaned</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">map_points</span><span class="p">(</span><span class="n">yield_clean_boarder</span><span class="p">,</span><span class="w"> </span><span class="s2">"Yld_Vol_Dr"</span><span class="p">,</span><span class="w"> </span><span class="s2">"Yield, No Boarders"</span><span class="p">)</span><span class="w">
</span><span class="n">yield_plot_comp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tmap_arrange</span><span class="p">(</span><span class="n">yield_plot_orig</span><span class="p">,</span><span class="w"> </span><span class="n">yield_plot_boarder_cleaned</span><span class="p">,</span><span class="w"> </span><span class="n">ncol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">nrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w">
</span><span class="n">yield_plot_comp</span><span class="w">
</span></code></pre></div></div>

<p><img src="../fig/rmd-unnamed-chunk-16-1.png" title="plot of chunk unnamed-chunk-16" alt="plot of chunk unnamed-chunk-16" width="612" style="display: block; margin: auto;" /></p>

<p>Here again, we also check the distribution of cleaned yield by making a histogram.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">hist</span><span class="p">(</span><span class="n">yield_clean_boarder</span><span class="o">$</span><span class="n">Yld_Vol_Dr</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="../fig/rmd-unnamed-chunk-17-1.png" title="plot of chunk unnamed-chunk-17" alt="plot of chunk unnamed-chunk-17" width="612" style="display: block; margin: auto;" /></p>

<p>Looking at both this histogram and the several very red dots in our de-boardered yield map we see that there are still a lot of very high observations so we need to proceed to step two which will clean our observations based on how far they are from the mean of the observations.</p>

<h3 id="data-cleaning-2-taking-out-outliers-far-from-the-mean">Data cleaning #2: Taking out outliers far from the mean</h3>

<p>Even if we don‚Äôt know the source of error, we can tell that some observations
are incorrect just because they are far too small or too large.  How can we
remove these in an objective, automatic way?  For yield and our other variables,
we will calculate the <a href="https://en.wikipedia.org/wiki/Standard_deviation">standard deviation</a>
to get an idea of how much the observations tend to be different from the mean.
Then, we will remove observations that are three standard deviations higher or
lower than the mean.  If the data followed a normal distribution (<em>i.e</em> a bell
curve), this would eliminate about one in 1000 data points.  In a real dataset,
we can be fairly certain that those points are errors.  Our cutoff of three
standard deviations is arbitrary, which is why we have looked at histograms of
the data to help confirm that our cutoff makes sense.</p>

<p>In the next few steps, we use <code class="highlighter-rouge">sd</code> and <code class="highlighter-rouge">mean</code> to calculate the standard
deviation and mean of the yield distribution, respectively. Then we remove the
yield observations that are greater than mean + 3*sd or less than mean - 3*sd.</p>
:ET