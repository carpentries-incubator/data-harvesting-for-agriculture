---
title: "Ag Carpentry - Data Cleaning and Aggregation"
author: "Aolin Gong"
date: "11/4/2019"
output: html_document
include_overview: true
questions:
 - Why is it important to clean the data before proceeding with analysis?
 - How can I quickly and efficiently identify problems with my data?
 - How can identify and remove incorrect values from my dataset?
objectives:
 - Confirm that data are formatted correctly
 - Enumerate common problems encountered with data formatting.
 - Visualize the distribution of recorded values
 - Identify and remove outliers in a dataset using R and QGIS
 - Correct other issues specific to how data were collected
keypoints:
 - Comparison operators such as `>`, `<`, and `==` can be used to identify values that exceed or equal certain values.
 - All the cleaning in the arcgis/qgis can be done by r, but we need to check the updated shapefile in Arcgis/qgis. Including removing observations that has greater than 2sd harvester speed, certain headlands, or being too close to the plot borders
 - The `filter` function in `dplyr` removes rows from a data frame based on values in one or more columns.
source: Rmd
---

```{r setup, include=FALSE}
library(sf)
library(fasterize)
library(gstat)
library(raster)
library(rjson)
library(httr)
library(rgdal)
library(rgeos)
library(maptools)
library(knitr)
library(tmap)
library(ggplot2)
library(gridExtra)
library(measurements)
library(knitr)
library(httr)
source("../bin/chunk-options.R")
source("./functions.R")
```

<font color="magenta"> NOTE: `clean_sd` gets called in the previous lecture so we should talk about data cleaning then or move it to here</font>

## Data cleaning and aggregation in the DIFM project

After harvesting, we collect all the data needed for analysis, and in advance of
running analysis, we clean and organize the data in order to remove machinary
error and such. The common data that we collect for analysis includes yield
(dry), seeding rate as-planted, nitrogen rate as-applied, electronic
conductivity (EC), SSURGO, soil test, weather, etc. In particular, we need to
clean yield data, as-planted data, as-applied data, and sometimes EC data. For
public data, we simply import them into our aggregated data set
without cleaning, since they have already been cleaned before being released to
the public.

## Introduction to data cleaning

Data cleaning is the process of removing or correcting errors in a dataset, and
is very important to do before any sort of analysis.  For example, say you were
manually entering yield values into a spreadsheet, and then wanted to take the
average of all values entered.  If you accidentally typed an extra zero into
some of the cells, the average that you calculate is going to be much higher
than the true average.

```{r}
real_data <- c(900, 450, 200, 320)
error_data <- c(900, 4500, 200, 320)
mean(real_data)
mean(error_data)
```

Therefore, we want to check for values like this before we do anything else.  If
the values were manually entered and the intended value is obvious, they can be
manually corrected.  For larger scale datasets, however, it is often most
practical to discard problematic data.

For example, we can plot our `error_data` and look for values that may look off:

```{r}
plot(error_data) # use plot function on error rate
```
By eye we can see the 2nd measurement (at `index = 2`) looks a little fishy.  In this case
we might want to apply a cut-off in our data so that we ignore all measurements above a
certain threshold when we do calculations like taking the mean of our data.

One way to do this is by setting any "weird" values to `NA`:

```{r}
error_data[error_data > 2000] <- NA # set any values bigger than 2000 to the NA tag
error_data
```

Now we can take a mean, with removing `NA`'s as we do it and recover a mean that is closer to the correct value:
```{r}
mean(error_data, na.rm=TRUE)
```


Data cleaning is a major reason why there needs to be good communication between
data scientists and end users, in agriculture or any other discipline.  As the person
who generates the data, you know best where the likely sources of error might be.
Those sources of error might be something that someone who sits behind a computer
all day would never think of.  You also know best what values are reasonable,
and what values are suspiciously high or low.

For different types of data, we have different ways to clean them. Here are the
main concerns of the original data for the major variables:

Yield, as-planted, and as-applied data:

* We remove observations where the harvester/planter/applicator is moving too slow or too fast.
* We remove observations on the edges of the plot.
* We remove observations that are below or above three standard deviations from the mean.
* We then aggregate them onto our units of observation. <font color="magenta">Do they know what these units are? I don't have a frame of refrence for this terminology</font>

<font color="magenta">This comes up in the 03 or 04 but we need to define what a standard devation is</font>
 
*For aggregation, we need to generate subplots (units of observation) of the
original trial design, and then aggregate the cleaned datasets for different
variables onto the subplots.  Once we have one value per variable per subplot,
we can begin examining the relationships between the variables.*

<font color="magenta">Have steps listed somewhere before we start</font>

## Step 1: Importing and transforming our shapefile datasets

<!-- Let's apply this data-cleaning thinking to measurements of yields.  First, we will aggregate our yields into a grid overlayed on top of our boundary shapefile and look for measurements on this grid that seem too low or to high, and flag these as "outliers" of our dataset in our data-cleaning process. -->

The first step is to read in our boundary and abline shape files and transform them to UTM for later use.  Let's do this step-by-step, starting with reading in the boundary shapefile and projecting it:

```{r boundary}
boundary <- read_sf("data/boundary.gpkg")
```
What is the current coordinate reference system of this object?
```{r boundary2}
st_crs(boundary)
```
Let's transform it to the UTM projection & check out its new coordinate reference system:
```{r boundary3}
boundary_utm <- st_transform_utm(boundary)
st_crs(boundary_utm)
```
Now we can see that the `+proj=longlat` has changed to `+proj=utm` and gives us that we are in UTM zone #17.

In the last episode, we also imported our trial design, which we will do again here:
```{r}
trial <- read_sf("data/trial.gpkg")
```

Let's look at the coordinate reference system here as well:
```{r}
st_crs(trial)
```
<font color="magenta">Do we have a figure showing lat/long to UTM coversion somewhere?  I can add this</font>

Our file is already in the UTM projection, but if we have one that is not we can convert this as well with `trial_utm <- st_transform_utm(trial)`.  For the sake of naming, we'll rename it as `trial_utm`:
```{r}
trial_utm <- trial
```

> ## Exercise: Transform the yield data
> Read in the yield shape file, look at its current CRS and transform it into the UTM projection.  Call this new, transformed variable `yield_utm`.
>
> > ## Solution
> > First, load the data:
> > ```{r}
> > yield <- read_sf("data/yield.gpkg")
> > ```
> > Then take a look at the coordinate system:
> > ```{r}
> > st_crs(yield)
> > ```
> > And finally transform into UTM:
> > ```{r}
> > yield_utm <- st_transform_utm(yield)
> > ```
> >
> {: .solution}
{: .challenge}



Finally, let's transform our abline file.  We read in the file:
```{r}
abline = st_read("data/abline.gpkg")
```
Check out its current coordinate reference system:
```{r}
st_crs(abline)
```
And transform it to UTM:
```{r}
abline_utm = st_transform_utm(abline)
```

## Step 2: Clean the yield data

Now that we have our shapefiles in the same UTM coordinate system reference frame, we will apply some of our knowledget of data cleaning to take out weird observations. We know we have "weird" measurements by looking at a histogram of our yield data:

```{r}
hist(yield_utm$Yld_Vol_Dr)
```

The fact that this histogram has a large tail where we see a few measurements far beyond the majority around 250 means we know we have some weird data points.

We will take out these weird observations in two steps:
  1. First, we will take out observations we *know* will be weird because they are taken from the edges of our plot.
  1. Second, we will take out observations that are too far away from where the majority of the other yield measurements lie.

Let's go through these one by one.

### Data cleaning #1: Taking out boarder observations

We need to remove the yield observations that are on the border of the plots,
and also at the end of the plots.  The reason for this is that along the edge
of a plot, the harvester is likely to measure a mixture of two plots,
and therefore the data won't be accurate for either plot.  Additionally,
plants growing at the edge of the field are likely to suffer from wind and other
effects, lowering their yields.

We add a 15 <font color="magenta">meter??</font> boarder.

```{r}
yield_clean_boarder <- clean_buffer(trial_utm, 15, yield_utm)
```

Let's use our side-by-side plotting we did in the previous episode to compare our original and boarder-yield cleaned yield maps:

```{r}
yield_plot_orig <- map_points(yield_utm, "Yld_Vol_Dr", "Yield, Orig")
yield_plot_boarder_cleaned <- map_points(yield_clean_boarder, "Yld_Vol_Dr", "Yield, No Boarders")
yield_plot_comp <- tmap_arrange(yield_plot_orig, yield_plot_boarder_cleaned, ncol = 2, nrow = 1)
yield_plot_comp
```

Here again, we also check the distribution of cleaned yield by making a histogram.

```{r}
hist(yield_clean_boarder$Yld_Vol_Dr)
```

Looking at both this histogram and the several very red dots in our de-boardered yield map we see that there are still a lot of very high observations so we need to proceed to step two which will clean our observations based on how far they are from the mean of the observations.

### Data cleaning #2: Taking out outliers far from the mean

Even if we don't know the source of error, we can tell that some observations
are incorrect just because they are far too small or too large.  How can we
remove these in an objective, automatic way?  For yield and our other variables,
we will calculate the [standard deviation](https://en.wikipedia.org/wiki/Standard_deviation)
to get an idea of how much the observations tend to be different from the mean.
Then, we will remove observations that are three standard deviations higher or
lower than the mean.  If the data followed a normal distribution (i.e a bell
curve), this would eliminate about one in 1000 data points.  In a real dataset,
we can be fairly certain that those points are errors.  Our cutoff of three
standard deviations is arbitrary, which is why we have looked at histograms of
the data to help confirm that our cutoff makes sense.

In the next few steps, we use `sd` and `mean` to calculate the standard
deviation and mean of the yield distribution, respectively. Then we remove the
yield observations that are greater than mean + 3\*sd or less than mean - 3\*sd.  We use
the `clean_sd` from our `functions.R`:

```{r 3sd}
yield_clean <- clean_sd(yield_clean_boarder, yield_clean_boarder$Yld_Vol_Dr)
```

Here again, we check the distribution of cleaned yield after taking out the
yield observations that are outside the range of three standard deviations from
the mean.

```{r view the distribution of cleaned yield data}
hist(yield_clean$Yld_Vol_Dr)
```

<!-- 
This looks a lot more sensible!  We can double check by comparing all the stages of our data cleaning on a map:
```{r}
yield_plot_orig <- map_points(yield_utm, "Yld_Vol_Dr", "Yield, Orig")
yield_plot_boarder_cleaned <- map_points(yield_clean_boarder, "Yld_Vol_Dr", "Yield, No Boarders")
yield_plot_clean <- map_points(yield_clean, "Yld_Vol_Dr", "Yield, Cleaned")
yield_plot_comp_final <- tmap_arrange(yield_plot_orig, yield_plot_boarder_cleaned, yield_plot_clean, ncol = 3, nrow = 1)
yield_plot_comp_final
```
-->

This looks a lot more sensible!  We can double check by looking at our final, cleaned yield map:
```{r}
yield_plot_clean <- map_points(yield_clean, "Yld_Vol_Dr", "Yield, Cleaned")
yield_plot_clean
```

> ## Discussion
> What do you think could have caused these outliers (extreme values)?  If you
were working with yield data from your own fields, what other sources of error
might you want to look for?
>
{: .callout}

> ## Exercise: Cleaning Nitrogen from asapplied
> 
> Import the `asapplied.gpkg` shapefile for and clean the nitrogen application data.
> 1. Remove observations from the
buffer zone, <font color="magenta">JPN: probably we need a different buffer length for this since I'm getting plots that have taken too much out in the boundary-removal process</font>
> 2. as well as observations more then three standard deviations from
the mean.
>
> > ## Solution
> > Load the data
> > ```{r}
> > nitrogen <- read_sf("data/asapplied.gpkg")
> > ```
> > Check CRS
> > ```{r}
> > st_crs(nitrogen)
> > ```
> > Since its in Lat/Long we have to transform it:
> > ```{r}
> > nitrogen_utm = st_transform_utm(nitrogen)
> > ```
> > Clean boarder:
> > ```{r}
> > nitrogen_clean_boarder <- clean_buffer(trial_utm, 15, nitrogen_utm)
> > ```
> > Check out our progress with a plot:
> > ```{r}
> > nitrogen_plot_orig <- map_points(nitrogen_utm, "Rate_Appli", "Nitrogen, Orig")
> > nitrogen_plot_boarder_cleaned <- map_points(nitrogen_clean_boarder, "Rate_Appli", "Nitrogen, No Boarders")
> > nitrogen_plot_comp <- tmap_arrange(nitrogen_plot_orig, nitrogen_plot_boarder_cleaned, ncol = 2, nrow = 1)
> > nitrogen_plot_comp
> > ```
> > Clean by standard deviation:
> > ```{r}
> > nitrogen_clean <- clean_sd(nitrogen_clean_boarder, nitrogen_clean_boarder$Rate_Appli)
> > ```
> > Plot our final result on a map:
> > ```{r}
> > nitrogen_plot_clean <- map_points(nitrogen_clean, "Rate_Appli", "Nitrogen, Cleaned")
> > nitrogen_plot_clean
> > ```
> > And as a histogram:
> > ```{r}
> > hist(nitrogen_clean$Rate_Appli)
> > ```
> >
> {: .solution}
{: .challenge}

nitrogen <- clean_sd(nitrogen, nitrogen$Rate_Appli)
