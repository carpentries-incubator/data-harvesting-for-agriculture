---
title: "Ag Carpentry - Weather and Soil Data"
author: "Brittani"
date: "10/19/2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#opts_knit$set(root.dir = "~/Box/Data Carpentry for Agronomy/DIFM Code and Information/hord f98") # delete once we have data in data folder
```

```{r packages, message = FALSE}
library("sf")
library("fasterize")
library("raster")
library("rjson")
library("httr")
library("rgdal")
library(rgeos)
library(maptools)
library(knitr)
require("tmap")
require("ggplot2")
require("gridExtra")
library("daymetr")
library("readr")
library("measurements")
library("FedData")
```

```{r datasets, echo = FALSE}
boundary <- read_sf("~/Box/Data Carpentry for Agronomy/DIFM Code and Information/hord f98/boundary.gpkg") # correct the location once we have data folder
```


####Motivating Questions:
- What publicly available datasets exist for my field?
- What tools are available to manipulate dates and data frames in R?

####Objectives:
- Describe the format of public weather and soil datasets
- Import weather data from the internet, eg. daymetr
- Access to elevation and ssurgo data with higher resolution
- Derive topography data from elevation data
- Use the `lubridate` package to work with dates.
- Use the `dplyr` package to manipulate data frames (spreadsheet-like data).

####Keypoints:
- The `daymetr` package is helpful for downloading daily weather data from Daymet.
- The `FedData` package can be used for downloading SSURGO soil data.
- Functions in the R base and `lubridate` package are helpful for working with
dates.
- The `dplyr` package makes it easier to filter a data frame, add new columns,
and summarize statistics after grouping by one or more variables.

#Bringing in the data from geospatial exercises

#Daymet Weather Data

The Oak Ridge National Laboratory produces a datset called
[Daymet](https://daymet.ornl.gov/) which contains predicted weather observations
on a one meter grid. These data come from weather station climate observations
in a climate model for prediction and include variables such as precipitation,
snow water equivalent, temperature, day length, solar radians, and vapor pressure. 

There is an R package called `daymetr` that downloads the daymet weather data
within the R environment. For a single point, you can use the command
`download_daymet()`. If you want to download the data for a set of points, there
is also the command `download_daymet_batch()` which takes a CSV file of the
points in lat/long.

We will use the mean latitude and longitude values from the bounding box as our
point for the weather data. This should be a point near the middle of the field.
We also call the site `Field1`, but this will be the name of a specific field if
you use it in the future. We can choose the start and end years. If the data is
not available for the year you request, an error will be reported. We choose
2000 to 2018 for this example; later we will use the historical data for
comparison. The final argument `internal = TRUE` means that the daymet data is
brought into the R environment rather than being saved in your working directory.

```{r location}
bbox <- as.matrix(st_bbox(boundary))
lon <- mean(bbox[c(1,3),])
lat <- mean(bbox[c(2,4),])
```

```{r daymetr}
weather <- download_daymet(site = "Field1", lat = lat, lon = lon,
                           start = 2000, end = 2018, internal = TRUE)
```

The object `weather` is a list of 7 objects, the last of is the data. Much like
columns in a data frame, list elements can be accessed using `$`, so we can do
`weather$data` to see the data.

*Exercise 1:* Explore the weather data

  1. Grab this object and save it as `weather_data`.
  2. How is the date reported? 
  3. What other variables exist?
  4. What are the units for the different variables?
  *Remember:* Sometimes you need to use a search engine to understand what objects are created from a specific R function. 


*Exercise 1 Solutions*

```{r weatherdata}
weather_data <- weather$data
summary(weather_data)
```

The date is reported as the year and day of the year. Other variables include
day length, precipitation, solar radiation, snow water equivalent, maximum
temperature, minimum temperature, and vapor pressure. The units for the
variables are given after the variable name. For example, day length is in
seconds and solar radiation is in watts per square meter. While precipitation
and temperature have intuitive names, vapor pressure and snow water equivalent
are not so apparent. Use the `datmetr` 
[vignette](https://cran.r-project.org/web/packages/daymetr/vignettes/daymetr-vignette.html)
to understand the meaning of these variables.

#Unit Conversions

Publicly available data are usually given in metric units as we saw in the
weather data above. We may want to have these data in imperial units as these
are the units we often are comparing in the United States; additionally, you may
know the value of crop requirements and threshholds in imperial units rather
than metric units. 

The package `measurements` in R converts observations from one unit to another.
The command ``conv_unit()` converts the column from one stated unit to another
unit. To see the possible units for a specific kind of measure, look at the
`conv_unit_options` for the specific measure you are converting (e.g. length,
area, weight, etc.). 

If we want to convert the daily precipitation from milimeters to inches, we will
first look at the unit options for length. Here we can see that inches are
"inch" and milimeters are "mm" in the `measurements` framework. We then use
`conv_unit()` with these arguments and our `prcp..mm.day.` column to create a
new column called `prec`.

```{r unitopts}
conv_unit_options$length
```

```{r convprec}
weather_data$prec <- conv_unit(weather_data$prcp..mm.day., "mm", "inch")
```

*Exercise 2:* Unit Conversions

  1. Look at the possible units for temperature in `measurements`. 
  2. Convert the two temperature variables into fahrenheit from celsius with the names `tmax` and `tmin`. 


*Execrise 2 Solutions*

```{r exercise1}
conv_unit_options$temperature
weather_data$tmax <- conv_unit(weather_data$tmax..deg.c., "C", "F")
weather_data$tmin <- conv_unit(weather_data$tmin..deg.c., "C", "F")
head(weather_data$tmax)
```

#Dates in data frames 

There is a class within R for dates. Once a column is of the `Date` class, we
can subset or order the dataset by time. `as.Date()` converts a column to a date,
but here if we try the command `weather_data$date <- as.Date(weather_data$yday)`,
we will receive an error saying an origin must be supplied. 

The function can see that the date is in days after some starting time or origin.
The name `yday` means this is the day of the year, so the origin should be the last
day of the previous year. There are multiple years in our data frame, so the origin
should change for each year. This is accomplished by pasting `weather_data$year-1`
and `"-12-31"` together using the function `paste0()` so that the origin is always
the last day of the previous year.  (Recall that `paste0` creates text by combining
text and/or numbers.)

```{r dateformat}
weather_data$date <- as.Date(weather_data$yday,
                             origin = paste0(weather_data$year - 1, "-12-31"))
head(weather_data$date)
```

We will also want to summarize the data by month, and to do that we will want a
column that just indicates month.  There is the `months` function in the R base:

```{r}
testdates <- as.Date(c("2019-10-03", "2019-06-17"))
months(testdates)
```

However, it outputs the months as plain text, which will get sorted
alphabetically by other functions later.  We would rather have the month as a
number, or as a factor where the order of the months is defined.  The
`lubridate` package has a function called `month` that can accomplish this.
If you need to perform more manipulation of dates or times, I suggest reading
more about `lubridate` at the [Tidyverse website](https://lubridate.tidyverse.org/).
The `::` operator specifies that we want to use a function from a particular
package.  You may need to run `install.packages("lubridate")` if you don't have
it installed already.

```{r}
lubridate::month(testdates)
lubridate::month(testdates, label = TRUE)
```

Now let's add month to our data frame.

```{r addmonth}
weather_data$month <- lubridate::month(weather_data$date, label = TRUE)
```

#Precipitation Graph

Perhaps you want to see what the weather this year was like compared to the average historic weather for the same area. We will make a graph showing the total monthly precipitation from 2018 compared to the average precipitation from the years 2000 to 2017. This is a common way to look at seasonal rainfall and allows us to look at the rainfall during the critical months of July and August.

Now, we need to sum the daily precipitation for each year and month combination. There is a package called `dplyr` that helps with many kinds of data manipulation. A popular task is to perform an action over a group. To specify the grouping variables, you use `group_by()` then add the additional command `summarise()` which defines the action. 

The next steps organize a dataframe for a graph of the average monthly precipitation from 2000 to 2017. 

  1. Create subset of `weather_data` taking out the 2018 observations.
  2. Create a monthly precipiation variable for each year and month combination called `prec_month` using the `dplyr` commands.
  3. Take an average of `prec_month` for each month.


*Steps*

  1. Create subset of `weather_data` taking out the 2018 observations.

```{r 00to17}
hist_data <- subset(weather_data,year != 2018)
```

  2. Create a monthly precipiation for each year called `prec_month` using the `dplyr` commands.

```{r monthyeardata}
by_month_year <- hist_data %>%
  dplyr::group_by(month, year) %>%
  dplyr::summarise(prec_month = sum(prec))
```

  3. Take an average of `prec_month` for each month.

```{r monthdata}
by_month <- by_month_year %>%
  dplyr::group_by(month) %>%
  dplyr::summarise(prec_avg = mean(prec_month))
```

*Exercise 3:* Using `dplyr`

  1. Create a new dataframe called `weather_2018` from the `weather_data` using only 2018 observations.
  2. Create a monthly precipiation variable for each month in 2018 called `prec_month` using the `dplyr`       commands.


*Exercise 3 Solutions*

```{r 2018data}
weather_2018 <- subset(weather_data, year == 2018)
by_month_2018 <- weather_2018 %>%
  dplyr::group_by(month) %>% 
  dplyr::summarise(prec_2018 = sum(prec))
```

We now have two separate dataframes `by_month_2018` and `by_month` with the rainfall for each month. We can use the common variable `month` to merge them into one dataframe with the average monthly rainfall and the 2018 monthly rainfall using the `merge()` function.

```{r plotdata}
prec_plot <- merge(by_month,by_month_2018, by = "month")
```

We will now use `ggplot` to create a graph with a bar representing the monthly precipitation in 2018 and a point with the average rainfall from 2000 to 2017. In the function `geom_bar()` `stat = identity` creates a bar graph where the height of the bar is the value of the variable rather than the count of the observations, the common use of a bar chart. 

```{r precmonthplot}
ggplot(prec_plot) + 
  geom_bar(aes(x = month, y = prec_2018), stat = 'identity') +
  geom_point(aes(month, prec_avg), show.legend = TRUE) +
  ggtitle("Field 1")
```

The most notable feature of the weather graph is the below average rainfall in July, the most critical growing period for corn. To understand whether this affected yield on the field, we woud also need to look at historic yield. But on your field, you will know those historic average and be able to have a pretty clear idea of weather impacted the average yield in a growing season. 

Another possible graph you could make with these data is on the accumulated GDD each month. 

#SSURGO Soil Data

The SSURGO data is probably a dataset you are familiar with already. You can obtain a soil description of your field on the Web Soil Survey website below. The SSURGO dataset has been developed over a century of surveying land and analyzing soil samples across the United States. While the website is one way to access the soil data, R also has a package called `FedData` that has a function `get_ssurgo()` for accessing the soil data in the R environment. 

https://websoilsurvey.sc.egov.usda.gov/App/WebSoilSurvey.aspx

The next line brings the SSURGO data into the R environment with the name `ssurgo` and the object `boundary` from the geospatial lesson. Note here that the class of `boundary` needs to be `spatial` rather than `sf`, so we trasnform the object with `as(boundary,"Spatial")`.

```{r ssurgo}
bound.sp <- as(boundary, "Spatial")
ssurgo <- get_ssurgo(bound.sp, "field1")
```

The downloaded `ssurgo` is a list with 2 objects, `spatial` and `tabular`. The `spatial` object contains the polygons of soil types for the field, and  `tabular` contains many dataframes with attributes collected for the soil and soil horizons. Note that these dataframes and their relationships with one another ar very complex. To use these data, you must carefully read the SSURGO documentation. Merging the dataframes to have one value of the attributes for each soil polygon requires reducing the dimension of the data, often by weighting the attributes by horizon depth. For an example of how this is done with clay, silt, and sand content, contact the workshop instructors. 

Let's make a map of the soil types on this field. First, we need to locate the part of `tabular` with the soil names; these can be found in `muaggatt`.

```{r names}
names <- ssurgo$tabular$muaggatt 
```

*Exercise 4*: What are the soil types present on the field as seen in `names`? Are the soil defined by anything other than the soil type?

*Exercise 4 Solution*

```{r soilnames}
names
```

Looking at `names` we can see there are eight types of soil on the field, and the dataframe reports areas with different slopes with different names. We often know the slope of the field, and so we may want to combine areas of the field with the same soil type and different slopes.

We need one dataframe with both the soil name and spatial data. We will merge the soil data and the spatial data by the `musym`. Note that in one of the dataframes the variable is capitalized and not in the other. We must rename the variable for consistency using `rename()` from `dplyr`.

```{r soilmerge}
spatial <- as(ssurgo$spatial, "sf")
spatial <- dplyr::rename(spatial, musym = MUSYM)
spatial <- merge(spatial, names, by = "musym")
head(spatial$muname)
```

*Exercise 5*: Create the Soil Map

Use `tmap` to make a map where the polygon color is informed by the soil names in `muname`. *Hint*: use `tm_polygons()`.


*Exercise 5 Solution*

```{r soilmap}
tm_shape(spatial) +
  tm_polygons('muname', title = "Soil Type") +
  tm_layout(legend.outside = TRUE, frame = FALSE) +
  tm_legend(text.size = 1,
            title.size = 1,
            width = 100,
            bg.color = "white")
```

The map shows that there are quite a few soil types on the field, and several show up in different section of the field. However, most of the soil are silt loam. It might be difficult to understand the different soils without more information about soil weathering and texture. This is also provided within SSURGO, and is likely, something you know about in your own field. 

