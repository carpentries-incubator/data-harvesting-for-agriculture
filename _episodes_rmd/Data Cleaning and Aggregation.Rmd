---
title: "Ag Carpentry - Data Cleaning and Aggregation"
author: "Aolin Gong"
date: "11/4/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Downloads/data carpentry - data cleaning") # ultimately delete this line
```

#### Motivating Questions:
- "Why is it important to clean the data before proceeding with analysis?"
- "How can I quickly and efficiently identify problems with my data?"
- "How can identify and remove incorrect values from my dataset?"
#### Objectives:
- "Confirm that data are formatted correctly"
- "Enumerate common problems encountered with data formatting."
- "Visualize the distribution of recorded values"
- "Identify and remove outliers in a dataset using R and QGIS"
- "Correct other issues specific to how data were collected"
#### Keypoints:
- "Comparison operators such as `>`, `<`, and `==` can be used to identify values that exceed or equal certain values."
- "All the cleaning in the arcgis/qgis can be done by r, but we need to check the updated shapefile in Arcgis/qgis. Including removing observations that has greater than 2sd harvester speed, certain headlands, or being too close to the plot borders"
- "The `filter` function in `dplyr` removes rows from a data frame based on values in one or more columns."

## Setup

First we will load the packages needed for this episode.  You should already
have these installed from the last episode.

```{r}
library(sf)
library(fasterize)
library(gstat)
library(raster)
library(rjson)
library(httr)
library(rgdal)
library(rgeos)
library(maptools)
library(knitr)
library(tmap)
library(ggplot2)
library(gridExtra)
```

Next, we will define a few functions that we will be using.  Feel free to copy
and paste this code into your script.

```{r functions}
st_over <- function(x, y) {
  sapply(sf::st_intersects(x, y), function(z)
    if (length(z) == 0)
      NA_integer_
    else
      z[1])
}

st_interpolate <- function(obj, v, rst, type = "idw") {
  obj_sp <- as(obj, "Spatial")
  
  fml <- as.formula(paste0(v, " ~ 1"))
  if (!(type %in% c("idw", "nng"))) {
    stop(paste0(
      "Type not implemented! ",
      "Choose between idw (inverse distance weighted), ",
      "and nng (nearest neighbor)."
    ))
  }
  
  if (type == "idw") {
    # Setting nmax to 5 and idp to 1 is an
    #  inverse weighted interpolation:
    gs <- gstat::gstat(formula = fml, locations = obj_sp,
                       nmax = 5, set = list(idp = 1))
  } else {
    # Setting nmax to 1 and idp to 0 is equivalent to
    #  nearest neighbor interpolation:
    gs <- gstat::gstat(formula = fml, locations = obj_sp,
                       nmax = 1, set = list(idp = 0))
  }
  
  x <- raster::interpolate(rst, gs)
  return(x)
}
```

In the last episode, we also imported our trial design.  We will include the
code again here, in case you are just joining us.

The following steps read in a trial design shapefile, transform the projection
of file to utm projection, and then save the file in a geopackage. In many
cases, the trial design shapefile is already in the correct form, and we are
just checking the file in advance of creating the subplots of the trial design.

```{r}
trial <- read_sf("hord_f98_trialdesign_2017.shp")

long2UTM <- function(long){
  utm <- (floor((long + 180)/6) %% 60) + 1
  return(utm)
}
utmzone <- long2UTM(mean(st_bbox(trial)[c(1,3)]))

projutm <- as.numeric(paste0("326", utmzone))

trialutm <- st_transform(trial, projutm)

st_write(trialutm, "trial.gpkg", layer_options = 'OVERWRITE=YES', update = TRUE)
```


## Introduction to data cleaning

Data cleaning is the process of removing or correcting errors in a dataset, and
is very important to do before any sort of analysis.  For example, say you were
manually entering yield values into a spreadsheet, and then wanted to take the
average of all values entered.  If you accidentally typed an extra zero into
some of the cells, the average that you calculate is going to be much higher
than the true average.

```{r}
real_data <- c(900, 450, 200, 320)
error_data <- c(900, 4500, 200, 320)
mean(real_data)
mean(error_data)
```

Therefore, we want to check for values like this before we do anything else.  If
the values were manually entered and the intended value is obvious, they can be
manually corrected.  For larger scale datasets, however, it is often most
practical to discard problematic data.  Below, we draw a plot to see the rough
distribution of values.  Then we identify a cutoff, and convert anything above
the cutoff to missing data.

```{r}
plot(error_data)
error_data[error_data > 2000] <- NA
error_data
```

Data cleaning is a major reason why there needs to be good communication between
data scientists and end users, in agriculture or any other discipline.  As the person
who generates the data, you know best where the likely sources of error might be.
Those sources of error might be something that someone who sits behind a computer
all day would never think of.  You also know best what values are reasonable,
and what values are suspiciously high or low.

## Data cleaning and aggregation in the DIFM project

After harvesting, we collect all the data needed for analysis, and in advance of
running analysis, we clean and organize the data in order to remove machinary
error and such. The common data that we collect for analysis includes yield
(dry), seeding rate as-planted, nitrogen rate as-applied, electronic
conductivity (EC), SSURGO, soil test, weather, etc. In particular, we need to
clean yield data, as-planted data, as-applied data, and sometimes EC data. For
the other types of data, we simply import them into our aggregated data set
without cleaning, since they have already been cleaned before being released to
the public.

For different types of data, we have different ways to clean them. Here are the
main concerns of the original data for the major varaibles:

Yield, as-planted, and as-applied data:

* We remove observations where the harvester/planter/applicator is moving too slow or too fast.
* We remove observations on the edges of the plot.
* We remove observations that are below or above three standard deviations from the mean.
* We then aggregate them onto our units of observation.

EC data:

* We remove EC observations that are below or above three standard deviations from the mean.
 
For aggregation, we need to generate subplots (units of observation) of the
original trial design, and then aggregate the cleaned datasets for different
variables onto the subplots.  Once we have one value per variable per subplot,
we can begin examining the relationships between the variables.

The following steps read in a boundary shapefile, and transform the projection
of file to utm projection for later use.
####read in and check the boundary file
```{r boundary}
boundary <- read_sf("boundary.gpkg")
boundary_utm<-st_transform(boundary, projutm)
```
After we read in the trial design file, we use a function to generate the subplots for this trial. Here I have not made the function yet, so we just read in the subplot generated with the supplementary code. (This part of the code can be later on transformed into a function as needed.)

In this step, we read in the subplots shapefile, and transform the projection of file to utm projection for later use.
###read in subplots
```{r read subplot}
subplots <- read_sf("hord_f98_subplots_2017.shp")
subplots_utm <- st_transform(subplots,projutm)
```

###visulization
Here, we plot the subplots we generated, note that the color pattern of the plot is determined by the ID number of the subplots, which is starts from 1, at the right upper corner. We can check how many units of observation we are generating with this subplots shapefile.

```{r vis subplots}
plot(subplots)
max(subplots$ID)
```

Then, we plot the geometry of subplots and boundary together, so that we get a better idea where the subplots are comparatively.

```{r visulize subplots}
plot(subplots_utm$geometry)
plot(boundary_utm$geom ,add=TRUE)
```

### Data Cleaning
After comfirming the subplots (ultimately the units of observations) we generated, we bring in the data sets for different variable in for cleaning. Here is an example of how we are cleaning our yield data,


When we bring in the edited yield data, we transform it into utm projection as well.
### Bring in yield data for cleaning
```{r yield}
yield <- read_sf("hord_f98_trialyield_2017.shp")
yield_utm<-st_transform(yield, projutm)
```

We check the distribution of the yield as we clean them to monitor the change made by each cleaning step. First, view the distrubution of the original data.
```{r vis yield data}
hist(yield_utm$Yld_Vol_Dr)
```

We need to remove the yield observations that are on the boarder of the plots, and also at the end of the plots.
### Remove yield observations on border and also the ends of plots with st_buffer 
#### Remove yield observations on boarder
The following code 'buffer<-st_buffer(trialutm,-4)' uses the function st_buffer to create a buffer for each plot, we set the buffer inside the trial plots to be 4 meters to the edges, and any yield observations that are within a 4-meter distance to the edge of the plots are considered on the boarder.

Then 'ov<-st_over(yield_utm,st_geometry(buffer)) & yield$out<- is.na(ov)' would mark any yield observations that are without the buffer as out. Finally, we remove the yield observations that are not in the buffer zone with the last line in this chunck.
```{r observations on boarder}
buffer<-st_buffer(trialutm,-4) # plots are 24 m wide and 2 yield passes
ov<-st_over(yield_utm,st_geometry(buffer))
yield$out<- is.na(ov) # demarcate the yield values removed
yield_clean<-subset(yield,out==FALSE)
```

Here again, we check the distribution of cleaned yield.
####Remove yield observations that are without three standard deviations
```{r view the distribution of yield data after taking out the yield points on the boader}
hist(yield_clean$Yld_Vol_Dr)
```
In the next few steps, we use 'sd<-sd(yield_clean\$Yld_Vol_Dr)' to calculate the standard deviation of the yield distribution, and use 'mean<-mean(yield_clean\$Yld_Vol_Dr)' to calcualte the mean of the yield distribution. Then we take the yield observations that are greater than mean + 3\*sd or less than mean - 3\*sd. 
```{r 3sd}
sd<-sd(yield_clean$Yld_Vol_Dr)
mean<-mean(yield_clean$Yld_Vol_Dr)
yield_clean<-subset(yield_clean,yield_clean$Yld_Vol_Dr>mean-3*sd & yield_clean$Yld_Vol_Dr<mean+3*sd)
```
Here again, we check the distribution of cleaned yield after taking out the yield observations that are without the range of three standard deviations.
```{r view the distribution of cleaned yield data}
hist(yield_clean$Yld_Vol_Dr)
```
The next line transforms the cleaned yield into utm projection
```{r reprojection of yield data}
yield_clean<-st_transform(yield_clean, projutm)
```
Finally, we save cleaned file into a geopackage.
####Save the cleaned yield data in a geopackage
```{r save yield}
st_write(yield_clean,"yield_clean.gpkg", layer_options = 'OVERWRITE=YES', update = TRUE)
```

###  Yield Interpolation/Aggregation
We interpolate the yield data onto the subplot we generated. There are multiple yield observations in each subplot. We take the median of these yield observations and assign the median to the according subplot. We would clean and aggregate the other variables the same way.

#Let us aggregate variables inside each subplot
#convert the yield data into sp
```{r sf2sp}
subplots_sp <- as(subplots_utm, "Spatial")
yield_sp <- as(yield_clean, "Spatial")
```
#Yield
```{r aggregate yield data}
#proj4string(yield_sp)<- CRS("+proj=longlat +datum=WGS84")
merge<-sp::over(subplots_sp,yield_sp,fn=median)
subplots_merge<-SpatialPolygonsDataFrame(subplots_sp, merge, match.ID = FALSE)
```

#ec
proj4string(ec)<- CRS("+proj=longlat +datum=WGS84")
merge2<-sp::over(subplots,ec,fn=median)
subplots@data<-cbind(merge2,subplots@data)

#asplanted and elevation
proj4string(asplanted)<- CRS("+proj=longlat +datum=WGS84")
merge3<-sp::over(subplots,asplanted,fn=median)
subplots@data<-cbind(merge3,subplots@data)
head(merge3)

#asapplied
proj4string(asapplied)<- CRS("+proj=longlat +datum=WGS84")
merge5<-sp::over(subplots,asapplied,fn=mean)
subplots@data<-cbind(merge5,subplots@data)
head(subplots@data)

#topography
topo <- spTransform(topography, CRS("+proj=longlat +datum=WGS84"))
slopemerge <- sp::over(subplots,topo,fn=median)
subplots@data <-cbind(slopemerge,subplots@data)

head(subplots@data)


####Other Files To Be Cleaned/Aggregated
1. As-applied Data
2. As-planted Data
4. EC Data
5. Elevation Data (from Internet or Trial Yield/As-planted file)
6. Topography Data (Slope and Aspect generated from Elevation Data)
7. SSURGO Data (Specify the soil content)
8. Weather Data (daily/weekly/monthly data)

####Supplementary Code/Function
1. Code to generate the suplots(unit of observations) of the orginal trial design.
2. Code to clean yield data.
3. Code to generate Topography data.
4. Code to process SSURGO data.
5. Code to download weather data.



